# ETL-процесса по обработке сделок клиентов: [Test DEMO]
by Oleksandr Husiev

## 1. Запуск вручну
Локальний запуск / інструкція:
    Рекомендовано створити віртуальне середовище (тестовано на conda) і встановити залежності:
        python -m pip install --upgrade pip
        python -m pip install -r requirements.txt

>Якщо використовуєте conda: 

    conda create -n etl python=3.10.19; 
    conda activate etl; pip install -r requirements.txt

Тестовано локально на miniconda `Python 3.10.19`. Запуск через термінал:
1. Перейти в теку проєкту
2. Запустити `python etl_weekly_trades.py`

За замовчуванням ETL pipeline буде запущено з наступними параметрами:

    out = run_etl(
        input_csv="trades (1) (2) (1).csv",
        sqlite_path="agg_result.db",
        table_name="agg_trades_weekly",
        compute_pnl=True,
        add_timestamp = False # Set False by default, if True adds timestamp to the file name top_clients.xlsx as in format '%Y%m%d_%H%M%S'
    )
- У цьому випадку запуску локально буде створено файл *.db в корені проєкту а також  `output/top_clients.xlsx`. Якшо з GitHub Actions, то лише файл бази даних, без Excel/CSV репорту (оскільки це прямо не зазначено в ТЗ, повернення цього файлу завжди можна додати через `etl_weekly_trades.yml`).
- Якщо спочатку запустити виконання скрипту з compute_pnl = False, а потім повторно з  compute_pnl = True, видаст помилку пыд час виконання оскілки буде несумісність рядків. Оскільки це виходить рамки ТЗ, цей момент не коригував, реалізація рішення if_exists="replace" уникла б цього, але разом з тим було б простішею.

## 2. Принцип роботи CI/CD

На прикладі тестового завдання, приписаний *.yml у .github/workflows задає параметри запуску віртуальної машини (runner) та отримання артефакту. Оскільки у цьому варіанті нічого не розгортається у реальну базу даних, можна сказати, що етап CD фактично відсутній.

У спрощеному вигляді принцип роботи CI/CD я розумію так:
- На кожен, наприклад push/workflow_dispatch, запускається workflow із прописаною job CI. Якщо CI падає, або інші умови що прописані заздалегіть для workflow не були результатом — коміт позначається червоним хрестиком, а CD не запускається.
- Інша job у файлі *.yml, умовно — CD, запускається коли тригери шо були виставлені ідентифікували свої умови, а також додаткові умови виставлені в пайплайні. Логіка цих додаткових умов, наприклад - для виконання CD має успішно завершитися CI (як от if ci.result == 'success').
- Результати (outputs) передаються з CI → CD через artifacts, щоб у на розгортання потрапила перевірена збірка.
- До того, продакшн може включати ручного підтвердження і branch protection за логікою шоб зламані пуші не потраплять у `main`.

## 3. Масштабування під 100+ млн рядків
### Какие технологии замените/добавите?
- Приділив би більше уваги векторизації операцій розрахунків і маніпуляцій, зокрема замінив би ітеративні обчислення на np.where(), pd.crosstab() тощо.
Також стосовно print() - напевно варто було б перейти на logging(), і вивід логів скоріше робив би англійською мовою, також напевно `pathlib` замісь `os`.
- Також варто було б подумати про chunked-обробку великих файлів через read_csv(..., chunksize=...), динамічний інпут, додати `.gitignore`, а також про специфіку розрахунку показників (типу unrealised/realised pnl), в плані коректності розрахунку до/після агрегації даних, та БД.
- Стосовно БД, поточна реалізація (append), хоч напевно не має сенсу в контексті тестового завдання, була зроблена як для скоріше демонстрації гіпотетичного накопичення історичних даних (щотижневі звіти). якщо йти за логікою оновлення щотижневих агрегатів, тоді заменовата краще розкоментувати replace або реалізувати оновлення за ключем (REPLACE INTO або if_exists="replace" + фільтр), + також додати перевірку на дублікати щоб уникнути дублювання при повторних записах. В цому плані законекчена бабліотека sqlite3 — як локальний варіант з python standard library перед, наприклад, Postgres/MySQL. 
- Також було б цікаво стосовно організації коду спробувати шось типу зробити кроки з list/tuple як малими функціями transform = (Step_1,Step_2 ...Step N); також замість використання requirements.txt подивтися детальніше в сторону системного опанування пакетного менеджера шо перевіряє сумісність, типу UV.

### Какую архитектуру ETL предложите?
Передусім я розглядав би це з точки зору ціловогу підходу до завдання та object-oriented approach. Якщо база даних тут реляційна, процес, уявляється, також включає інтеграцію з API та формулювання SQL-запитів, то логічно вибудовувати модульну структуру: набір незалежних блоків-функцій, які легко адаптувати або замінити, і також зручно для тестування та повторного використання, як частин ETL, спробувати ввести класи. А також:
- Ітеративне або паралельне виконання jobs (наприклад, за допомогою multiprocessing або GitHub Actions matrix strategy).
- Як додактвоий варіант, опція з точки зору self-hosted runner — тобто виконання GitHub Actions на власному сервері компанії. І також,  в теорії, у цьому контексті сам факт використання GitHub Actions як CI/CD-оркестратора (на противагу Jenkins, Argo, GitLab CI).

### Какие метрики мониторинга ETL вы бы внедрили?
1. Data-quality та performance-тести на етапі extraction: короткий summary по всіх полях (типи даних, наявність NaN/NaT, відсоток пропусків); метрики щодо потенційної кореляції пропусків між колонками; частотність появи різних типів значень, регулярність часових періодів.
2. Моніторинг часу та стабільності виконання пайплайну:
середній час виконання job (Avg job run time), середній час у черзі (Avg job queue time),
Job failure rate, Failed job retries, вимірювання використаних ресурсів середовища та агрегація цих даних не лише в GitHub, а й у внутрішній системі компанії. Також, потенційно, впровадження стандартизованих code & performance benchmarks, data validation library for Python - більш детально про застосування Pydantic.

### Где будут храниться входные и выходные данные?
Залежно від контексту, вхідні та вихідні дані можуть зберігатися по-різному. У тестовому прикладі невеликий вхідний файл розміщується локально в репозиторії, а кожен запуск runner у GitHub Actions створює тимчасове середовище, у якому виконується ETL-job і формується артефакт (наприклад, agg_result.db), що зберігається на стороні GitHub як артифакт; після завершення ран усі тимчасові дані видаляються. 

Стосовно БД, поточна реалізація (append), хоч напевно не має сенсу в контексті тестового завдання, була зроблена як для скоріше демонстрації гіпотетичного накопичення історичних даних (щотижневі звіти). якщо йти за логікою оновлення щотижневих агрегатів, тоді заменовата краще розкоментувати replace або реалізувати оновлення за ключем (REPLACE INTO або if_exists="replace" + фільтр), + також додати перевірку на дублікати щоб уникнути дублювання при повторних записах.

У більш просунутому сценарії вхідні дані, наприклад, підвантажуються пакетно або інкрементально з корпоративної бази чи хмарного сховища / object-based storage, а вихідні — записуються у "тестову" staging базу данних, чи експортуються у форматах CSV/Parquet для подальшої аналітики (звісно і в перспективі на production database). 


